{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING RETE NEURALE - classificazione captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform\n",
    "import torchvision.transforms as T\n",
    "# Define single transforms\n",
    "\n",
    "# Note: transforms can also be regular functions\n",
    "def normalize_(x):\n",
    "    # Set values\n",
    "    x[x > 0.5] = 1\n",
    "    x[x <= 0.5] = -1\n",
    "    # Return\n",
    "    return x\n",
    "\n",
    "\n",
    "normalize = normalize_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize 48x48\n",
    "def resize_(img):   \n",
    "    size = 48,48\n",
    "    img = img.resize(size)\n",
    "    return img\n",
    "\n",
    "resize = resize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Grayscale(num_output_channels=1),\n",
    "                       resize,\n",
    "                       T.ToTensor(),\n",
    "                       normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class map\n",
    "class_names = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I',9:\"J\",10:\"K\",11:\"L\",12:\"M\",13:\"N\",14:\"O\",15:\"P\",16:\"Q\",17:\"R\",18:\"S\",19:\"T\",20:\"U\",21:\"V\",22:\"W\",23:\"X\",24:\"Y\",25:\"Z\"};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image, ImageChops\n",
    "from random import shuffle\n",
    "from glob import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Convolutional layer\n",
    "class ConvLayer(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.add_module('conv', nn.Conv2d(in_features, out_features, kernel_size=3))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "# Convolutional layer\n",
    "class ConvLayerBN(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.add_module('conv', nn.Conv2d(in_features, out_features, kernel_size=3))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('bn', nn.BatchNorm2d(out_features))\n",
    "        self.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "# Define model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call parent\n",
    "        super().__init__()\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvLayerBN(1, 32),\n",
    "            ConvLayerBN(32, 128)\n",
    "        )\n",
    "        # Computing encoding size\n",
    "        self.convs.eval()\n",
    "        test_x = torch.zeros(1, 1, 48,48) #DIMENSIONE IN PIXEL\n",
    "        test_x = self.convs(test_x)\n",
    "        encoding_size = test_x.numel()\n",
    "        print(f\"Encoding size: {encoding_size}\")\n",
    "        # FC layers\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(encoding_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, len(class_names))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute output\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "root_dir = \"./output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. classes: 26\n",
      "Num. train samples: 42588\n",
      "Num. valid. samples: 14196\n",
      "Num. test samples: 14222\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "# Instantiate datasets\n",
    "letter_train_dataset = ImageFolder(os.path.join(root_dir,  \"train\"), transform) #, loader=loader)\n",
    "letter_val_dataset = ImageFolder(os.path.join(root_dir,  \"val\"), transform) #, loader=loader)\n",
    "letter_test_dataset = ImageFolder(os.path.join(root_dir,  \"test\"), transform) #, loader=loader)\n",
    "\n",
    "# Get number of classes (we'll need it in the model)\n",
    "num_classes = len(letter_train_dataset.classes)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Num. classes: {num_classes}\")\n",
    "print(f\"Num. train samples: {len(letter_train_dataset)}\")\n",
    "print(f\"Num. valid. samples: {len(letter_val_dataset)}\")\n",
    "print(f\"Num. test samples: {len(letter_test_dataset)}\")\n",
    "\n",
    "\n",
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=letter_train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=letter_val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=letter_test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48, 48])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_train_dataset[40][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "start_epoch = 0\n",
    "checkpoint = f\"checkpoint-{start_epoch}.pth\"\n",
    "use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding size: 12800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (convs): Sequential(\n",
       "    (0): ConvLayerBN(\n",
       "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): ConvLayerBN(\n",
       "      (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=12800, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=2048, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check checkpoint\n",
    "if use_checkpoint:\n",
    "    state_dict = torch.load(checkpoint)\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create model for training\n",
    "model = Model()\n",
    "if use_checkpoint:\n",
    "    model.load_state_dict(state_dict)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output size: torch.Size([1, 26])\n"
     ]
    }
   ],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = letter_train_dataset[0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "num_epochs = 36\n",
    "save_every = 6\n",
    "\n",
    "# Initialize training history\n",
    "\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "print(\"Start training\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy\n",
    "   \n",
    "    # Save checkpoint\n",
    "    if epoch > 0 and epoch % save_every == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        for k,v in state_dict.items():\n",
    "            state_dict[k] = v.cpu() #conversione CUDA - CPU\n",
    "        torch.save(state_dict, f\"checkpoint-{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model_name=\"captcha72k\"\n",
    "# Define root directory\n",
    "save_path = f\"./{model_name}{num_epochs}.pt\"\n",
    "modello_salvato = model.state_dict()\n",
    "torch.save(modello_salvato, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Testing del modello trainato. Prima si generano dei captcha casuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "#model.load_state_dict(torch.load(\"./modello30.pt\",map_location=dev))\n",
    "model.load_state_dict(torch.load(\"./checkpoint-24.pth\",map_location=dev)) \n",
    "model.eval()\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "prova_dir = \"./CaptchaTest/\"\n",
    "\n",
    "nome_file = \"UNICT72.png\"\n",
    "\n",
    "img = Image.open(os.path.join(prova_dir,nome_file))\n",
    "\n",
    "width, _ = img.size\n",
    "\n",
    "label = \"\"\n",
    "\n",
    "num_lett = int(width/200)\n",
    "for i in range(0,num_lett):\n",
    "    plt.figure(figsize=(2, 2)) #deve stare dentro per plottare le tre immmagini\n",
    "    img = Image.open(os.path.join(prova_dir, nome_file))\n",
    "    img = img.convert('1')\n",
    "    #crop ogni 200px 5 volte\n",
    "    #0,0 200,200 - 200,0 400,200 - 400,0 600, 200\n",
    "    current_box = img.crop((200*i,0,200*(i+1),200))\n",
    "\n",
    "    input = transform(current_box)\n",
    "    plt.imshow(current_box)\n",
    "    # Predict class\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input.unsqueeze(0).to(dev))\n",
    "    accuracy = output.softmax(1).max(1)\n",
    "    accuracy = accuracy[0].item()*100\n",
    "    _,pred = output.max(1)\n",
    "    pred = pred.item()\n",
    "    print(f\"Predicted: {class_names[pred]}  with accuracy of: {'%.2f'%(accuracy)}\")\n",
    "    #print(\"secondo\", f\"{'%.2f'%(secondo)}\")\n",
    "    #plt.show(input)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill with borders\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Define root directory\n",
    "prova_dir = \"F:/Programmazione/UO-Captcha-breaker/python-utils-scripts/test-folder/\"\n",
    "\n",
    "img = Image.open(prova_dir+\"preview7.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "img_with_border = ImageOps.expand(img,border=49,fill='black')\n",
    "plt.imshow(img_with_border)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio con test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "# Define root directory\n",
    "prova_dir = \"F:/Programmazione/UO-Captcha-breaker/\"\n",
    "\n",
    "\n",
    "#transform = T.Compose([T.Grayscale(num_output_channels=1),\n",
    "                       #T.ToTensor(),\n",
    "                       #normalize])\n",
    "\n",
    "\n",
    "#input, label = Image.open(prova_dir+\"TEST42rgb.png\")\n",
    "\n",
    "# Get random sample from test set\n",
    "#idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[0]\n",
    "\n",
    "# Normalize and show image\n",
    "#input_show = (input - input.min())/(input.max() - input.min())\n",
    "#plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "#plt.imshow(input_show)\n",
    "#plt.axis('off')\n",
    "\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input.unsqueeze(0).to(dev))\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a model for dog detection\n",
    "\n",
    "In fine-tuning, we will include layers from a pre-trained model into our own network, then train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the `features` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input\n",
    "alexnet.eval()\n",
    "test_x = torch.zeros(1, 3, 224, 224).to(dev)\n",
    "# Forward whole model\n",
    "print(alexnet(test_x).size())\n",
    "# Forward features only\n",
    "print(alexnet.features(test_x).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned model\n",
    "class FineTunedAlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load AlexNet model\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "        # Select feature extraction part\n",
    "        self.features = alexnet.features\n",
    "        self.fc1 = nn.Linear(256*6*6, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.output = nn.Linear(2048, 133)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.output(x),1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = FineTunedAlexNet()\n",
    "model = model.to(dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = dog_train_dataset[0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize training history\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "for epoch in range(100):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get random sample from test set\n",
    "idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[idx]\n",
    "# Normalize and show image\n",
    "input_show = (input - input.min())/(input.max() - input.min())\n",
    "plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input.unsqueeze(0).to(dev))\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using a pre-trained model as a feature extractor\n",
    "\n",
    "Fully-connected layer of models pre-trained on ImageNet can also work as generic image descriptors, because they compactly represent image content.\n",
    "\n",
    "Feature extraction means that we pass an image to a CNN model, but only get the output of a fully-connected layer, and use that output instead of the fully image. Then, we train a simpler classifier (SVM or MLP) on the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction in PyTorch may be tricky sometimes, depending on how the model is defined. We will see two methods to extract features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Modifying layers\n",
    "\n",
    "Let's see the Alexnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "alexnet.eval()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "alexnet(torch.zeros(1, 3, 224, 224).to(dev)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's suppose we want the output of the last FC layer (before classification). In practice, we want the output of layer `5` inside the `classifier` block. The problem is that `classifier` is a `Sequential`, so we cannot directly get the output of an intermediate layer.\n",
    "\n",
    "One solution is to modify the `classifier` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classifier block\n",
    "alexnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential children\n",
    "alexnet.classifier.children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert generator to list\n",
    "list(alexnet.classifier.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the final linear layer\n",
    "new_classifier_modules = list(alexnet.classifier.children())\n",
    "new_classifier_modules = new_classifier_modules[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the new list of modules\n",
    "new_classifier_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Sequential container\n",
    "new_classifier = nn.Sequential(*new_classifier_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the classifier\n",
    "alexnet.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "test_out = alexnet(torch.zeros(1, 3, 224, 224).to(dev))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep output for later\n",
    "features_1 = test_out.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, our `alexnet` model now returns directly the output of a fully-connected layer. However, this was easy because `classifier` is the last block of the model, so there's nothing after it that depends on it.\n",
    "\n",
    "In other cases, it may not be so easy. For example, consider the [`inception_v3`](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py) model.\n",
    "\n",
    "In that case, layers are defined as individual class properties, and the `forward()` method calls them in turn. If we want to extract features, we should modify the `forward()` method. It can be done, but it's not trivial, and there's a cleaner way: **hooks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Forward hooks\n",
    "\n",
    "Hooks are callback functions associated to `nn.Module`s, that are invoked during forward (_forward hook_) or during backpropagation (_backward hook_).\n",
    "\n",
    "When you _register_ a hook, you ask the model to call your function whenever a layer processes input data. Your function will receive the input and output of the module.\n",
    "\n",
    "Another way to implement feature extraction is to set up a forward hook on our target layer, get the layer's output, and save it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "alexnet.eval()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction class\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    # Constructor: receives model and target layer\n",
    "    def __init__(self, model, layer):\n",
    "        # Save model\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Internal variable to store target features\n",
    "        self.features = None\n",
    "        # Define hook\n",
    "        def forward_hook(module, input, output):\n",
    "            # Copy features\n",
    "            self.features = output.clone()\n",
    "        # Register hook\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        \n",
    "    # Function interface\n",
    "    def __call__(self, input):\n",
    "        with torch.no_grad():\n",
    "            # Forward through model\n",
    "            self.model(input)\n",
    "        # Return features\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get reference to target layer?\n",
    "\n",
    "Just traverse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extractor\n",
    "feat_extr = FeatureExtractor(alexnet, alexnet.classifier[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "test_out = feat_extr(torch.zeros(1, 3, 224, 224).to(dev))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare features\n",
    "features_2 = test_out\n",
    "print((features_1 - features_2).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our feature extraction to process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of features\n",
    "num_features = feat_extr(dog_train_dataset[0][0].unsqueeze(0).to(dev)).numel()\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrices (num_samples x num_features)\n",
    "datasets = {\"train\": dog_train_dataset, \"val\": dog_val_dataset, \"test\": dog_test_dataset}\n",
    "features = {\"train\": torch.Tensor(len(dog_train_dataset), num_features),\n",
    "            \"val\":   torch.Tensor(len(dog_val_dataset), num_features),\n",
    "            \"test\":  torch.Tensor(len(dog_test_dataset), num_features)\n",
    "           }\n",
    "labels = {\"train\": torch.LongTensor(len(dog_train_dataset)),\n",
    "          \"val\":   torch.LongTensor(len(dog_val_dataset)),\n",
    "          \"test\":  torch.LongTensor(len(dog_test_dataset))\n",
    "         }\n",
    "# Fill the features for each split\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"Processing {split} split\")\n",
    "    # Process each sample in the split\n",
    "    for i in range(len(datasets[split])):\n",
    "        # Get sample\n",
    "        sample,label = datasets[split][i]\n",
    "        # Compute features\n",
    "        sample = sample.unsqueeze(0).to(dev)\n",
    "        feats = feat_extr(sample)\n",
    "        # Copy features\n",
    "        features[split][i] = feats\n",
    "        labels[split][i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data formatted as in our initial examples with linear regression and classification. However, we can still use the standard `DataLoader` interface, by wrapping our matrices as `TensorDataset` objects. In a `TensorDataset`, you can pass any kind of tensors as source data, and sample selection is performed by indexing the first dimension (in our case, rows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from torch.utils.data import TensorDataset\n",
    "# Prepare tensor datasets\n",
    "tensor_datasets = {\n",
    "    split: TensorDataset(features[split], labels[split]) for split in features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=tensor_datasets[\"train\"], batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=tensor_datasets[\"val\"],   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=tensor_datasets[\"test\"],  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a linear classifier on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = nn.Linear(num_features, num_classes)\n",
    "model = model.to(dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = tensor_datasets[\"train\"][0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training history\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "for epoch in range(100):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get random sample from test set\n",
    "idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[idx]\n",
    "# Normalize and show image\n",
    "input_show = (input - input.min())/(input.max() - input.min())\n",
    "plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "# Extract features\n",
    "input = feat_extr(input.unsqueeze(0).to(dev))\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
