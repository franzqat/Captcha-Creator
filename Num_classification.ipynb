{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING RETE NEURALE - classificazione captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform\n",
    "import torchvision.transforms as T\n",
    "# Define single transforms\n",
    "\n",
    "# Note: transforms can also be regular functions\n",
    "def normalize_(x):\n",
    "    # Set values\n",
    "    x[x > 0.5] = 1\n",
    "    x[x <= 0.5] = -1\n",
    "    # Return\n",
    "    return x\n",
    "\n",
    "\n",
    "normalize = normalize_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize 48x48\n",
    "def resize_(img):   \n",
    "    size = 48,48\n",
    "    img = img.resize(size)\n",
    "    return img\n",
    "\n",
    "resize = resize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Grayscale(num_output_channels=1),\n",
    "                       resize,\n",
    "                       T.ToTensor(),\n",
    "                       normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class map\n",
    "class_names = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I',9:\"J\",10:\"K\",11:\"L\",12:\"M\",13:\"N\",14:\"O\",15:\"P\",16:\"Q\",17:\"R\",18:\"S\",19:\"T\",20:\"U\",21:\"V\",22:\"W\",23:\"X\",24:\"Y\",25:\"Z\"};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image, ImageChops\n",
    "from random import shuffle\n",
    "from glob import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "root_dir = \"./output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Convolutional layer\n",
    "class ConvLayer(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.add_module('conv', nn.Conv2d(in_features, out_features, kernel_size=3))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "# Convolutional layer\n",
    "class ConvLayerBN(nn.Sequential):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.add_module('conv', nn.Conv2d(in_features, out_features, kernel_size=3))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('bn', nn.BatchNorm2d(out_features))\n",
    "        self.add_module('pool', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "# Define model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call parent\n",
    "        super().__init__()\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvLayerBN(1, 32),\n",
    "            ConvLayerBN(32, 128)\n",
    "        )\n",
    "        # Computing encoding size\n",
    "        self.convs.eval()\n",
    "        test_x = torch.zeros(1, 1, 48,48) #DIMENSIONE IN PIXEL\n",
    "        test_x = self.convs(test_x)\n",
    "        encoding_size = test_x.numel()\n",
    "        print(f\"Encoding size: {encoding_size}\")\n",
    "        # FC layers\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(encoding_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, len(class_names))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute output\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "# Instantiate datasets\n",
    "letter_train_dataset = ImageFolder(os.path.join(root_dir,  \"train\"), transform) #, loader=loader)\n",
    "letter_val_dataset = ImageFolder(os.path.join(root_dir,  \"val\"), transform) #, loader=loader)\n",
    "letter_test_dataset = ImageFolder(os.path.join(root_dir,  \"test\"), transform) #, loader=loader)\n",
    "\n",
    "# Get number of classes (we'll need it in the model)\n",
    "num_classes = len(letter_train_dataset.classes)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Num. classes: {num_classes}\")\n",
    "print(f\"Num. train samples: {len(letter_train_dataset)}\")\n",
    "print(f\"Num. valid. samples: {len(letter_val_dataset)}\")\n",
    "print(f\"Num. test samples: {len(letter_test_dataset)}\")\n",
    "\n",
    "\n",
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=letter_train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=letter_val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=letter_test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "start_epoch = 0\n",
    "checkpoint = f\"checkpoint-{start_epoch}.pth\"\n",
    "use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check checkpoint\n",
    "if use_checkpoint:\n",
    "    state_dict = torch.load(checkpoint)\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create model for training\n",
    "model = Model()\n",
    "if use_checkpoint:\n",
    "    model.load_state_dict(state_dict)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = letter_train_dataset[0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "num_epochs = 30\n",
    "save_every = 1\n",
    "\n",
    "# Initialize training history\n",
    "\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy\n",
    "   \n",
    "    # Save checkpoint\n",
    "    if epoch % save_every == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        for k,v in state_dict.items():\n",
    "            state_dict[k] = v.cpu() #conversione CUDA - CPU\n",
    "        torch.save(state_dict, f\"checkpoint-{epoch}.pth\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "# Define root directory\n",
    "save_path = f\"./modello{num_epochs}.pt\"\n",
    "modello_salvato = model.state_dict()\n",
    "torch.save(modello_salvato, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Testing del modello trainato. Prima si generano dei captcha casuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding size: 12800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (convs): Sequential(\n",
       "    (0): ConvLayerBN(\n",
       "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): ConvLayerBN(\n",
       "      (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=12800, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=2048, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"./modello30.pt\",map_location=dev))\n",
    "model.eval()\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: R  with accuracy of: 55.12\n",
      "Predicted: Y  with accuracy of: 54.73\n",
      "Predicted: T  with accuracy of: 98.88\n",
      "Predicted: H  with accuracy of: 92.09\n",
      "Predicted: N  with accuracy of: 88.92\n",
      "Predicted: G  with accuracy of: 97.52\n",
      "Predicted: G  with accuracy of: 99.26\n",
      "Predicted: D  with accuracy of: 99.89\n",
      "Predicted: N  with accuracy of: 92.13\n",
      "Predicted: F  with accuracy of: 70.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define root directory\n",
    "prova_dir = \"./CaptchaTest/\"\n",
    "\n",
    "img = Image.open(os.path.join(prova_dir,\"AYTHNGGDNF498.png\"))\n",
    "\n",
    "width, _ = img.size\n",
    "\n",
    "label = \"\"\n",
    "\n",
    "num_lett = int(width/200)\n",
    "for i in range(0,num_lett):\n",
    "    plt.figure(figsize=(2, 2)) #deve stare dentro per plottare le tre immmagini\n",
    "    img = Image.open(os.path.join(prova_dir,\"AYTHNGGDNF498.png\"))\n",
    "    img = img.convert('1')\n",
    "    #crop ogni 200px 5 volte\n",
    "    #0,0 200,200 - 200,0 400,200 - 400,0 600, 200\n",
    "    current_box = img.crop((200*i,0,200*(i+1),200))\n",
    "\n",
    "    input = transform(current_box)\n",
    "    plt.imshow(current_box)\n",
    "    # Predict class\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input.unsqueeze(0).to(dev))\n",
    "    accuracy = output.softmax(1).max(1)\n",
    "    accuracy = accuracy[0].item()*100\n",
    "    _,pred = output.max(1)\n",
    "    pred = pred.item()\n",
    "    print(f\"Predicted: {class_names[pred]}  with accuracy of: {'%.2f'%(accuracy)}\")\n",
    "    #print(\"secondo\", f\"{'%.2f'%(secondo)}\")\n",
    "    #plt.show(input)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill with borders\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Define root directory\n",
    "prova_dir = \"F:/Programmazione/UO-Captcha-breaker/python-utils-scripts/test-folder/\"\n",
    "\n",
    "img = Image.open(prova_dir+\"preview7.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "img_with_border = ImageOps.expand(img,border=49,fill='black')\n",
    "plt.imshow(img_with_border)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio con test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "# Define root directory\n",
    "prova_dir = \"F:/Programmazione/UO-Captcha-breaker/\"\n",
    "\n",
    "\n",
    "#transform = T.Compose([T.Grayscale(num_output_channels=1),\n",
    "                       #T.ToTensor(),\n",
    "                       #normalize])\n",
    "\n",
    "\n",
    "#input, label = Image.open(prova_dir+\"TEST42rgb.png\")\n",
    "\n",
    "# Get random sample from test set\n",
    "#idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[0]\n",
    "\n",
    "# Normalize and show image\n",
    "#input_show = (input - input.min())/(input.max() - input.min())\n",
    "#plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "#plt.imshow(input_show)\n",
    "#plt.axis('off')\n",
    "\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input.unsqueeze(0).to(dev))\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a model for dog detection\n",
    "\n",
    "In fine-tuning, we will include layers from a pre-trained model into our own network, then train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the `features` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input\n",
    "alexnet.eval()\n",
    "test_x = torch.zeros(1, 3, 224, 224).to(dev)\n",
    "# Forward whole model\n",
    "print(alexnet(test_x).size())\n",
    "# Forward features only\n",
    "print(alexnet.features(test_x).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned model\n",
    "class FineTunedAlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load AlexNet model\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "        # Select feature extraction part\n",
    "        self.features = alexnet.features\n",
    "        self.fc1 = nn.Linear(256*6*6, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.output = nn.Linear(2048, 133)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.output(x),1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = FineTunedAlexNet()\n",
    "model = model.to(dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = dog_train_dataset[0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize training history\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "for epoch in range(100):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get random sample from test set\n",
    "idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[idx]\n",
    "# Normalize and show image\n",
    "input_show = (input - input.min())/(input.max() - input.min())\n",
    "plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input.unsqueeze(0).to(dev))\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using a pre-trained model as a feature extractor\n",
    "\n",
    "Fully-connected layer of models pre-trained on ImageNet can also work as generic image descriptors, because they compactly represent image content.\n",
    "\n",
    "Feature extraction means that we pass an image to a CNN model, but only get the output of a fully-connected layer, and use that output instead of the fully image. Then, we train a simpler classifier (SVM or MLP) on the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction in PyTorch may be tricky sometimes, depending on how the model is defined. We will see two methods to extract features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Modifying layers\n",
    "\n",
    "Let's see the Alexnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "alexnet.eval()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "alexnet(torch.zeros(1, 3, 224, 224).to(dev)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's suppose we want the output of the last FC layer (before classification). In practice, we want the output of layer `5` inside the `classifier` block. The problem is that `classifier` is a `Sequential`, so we cannot directly get the output of an intermediate layer.\n",
    "\n",
    "One solution is to modify the `classifier` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classifier block\n",
    "alexnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential children\n",
    "alexnet.classifier.children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert generator to list\n",
    "list(alexnet.classifier.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the final linear layer\n",
    "new_classifier_modules = list(alexnet.classifier.children())\n",
    "new_classifier_modules = new_classifier_modules[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the new list of modules\n",
    "new_classifier_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Sequential container\n",
    "new_classifier = nn.Sequential(*new_classifier_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the classifier\n",
    "alexnet.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "test_out = alexnet(torch.zeros(1, 3, 224, 224).to(dev))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep output for later\n",
    "features_1 = test_out.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, our `alexnet` model now returns directly the output of a fully-connected layer. However, this was easy because `classifier` is the last block of the model, so there's nothing after it that depends on it.\n",
    "\n",
    "In other cases, it may not be so easy. For example, consider the [`inception_v3`](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py) model.\n",
    "\n",
    "In that case, layers are defined as individual class properties, and the `forward()` method calls them in turn. If we want to extract features, we should modify the `forward()` method. It can be done, but it's not trivial, and there's a cleaner way: **hooks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Forward hooks\n",
    "\n",
    "Hooks are callback functions associated to `nn.Module`s, that are invoked during forward (_forward hook_) or during backpropagation (_backward hook_).\n",
    "\n",
    "When you _register_ a hook, you ask the model to call your function whenever a layer processes input data. Your function will receive the input and output of the module.\n",
    "\n",
    "Another way to implement feature extraction is to set up a forward hook on our target layer, get the layer's output, and save it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet = alexnet.to(dev)\n",
    "alexnet.eval()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction class\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    # Constructor: receives model and target layer\n",
    "    def __init__(self, model, layer):\n",
    "        # Save model\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Internal variable to store target features\n",
    "        self.features = None\n",
    "        # Define hook\n",
    "        def forward_hook(module, input, output):\n",
    "            # Copy features\n",
    "            self.features = output.clone()\n",
    "        # Register hook\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        \n",
    "    # Function interface\n",
    "    def __call__(self, input):\n",
    "        with torch.no_grad():\n",
    "            # Forward through model\n",
    "            self.model(input)\n",
    "        # Return features\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get reference to target layer?\n",
    "\n",
    "Just traverse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extractor\n",
    "feat_extr = FeatureExtractor(alexnet, alexnet.classifier[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test output\n",
    "test_out = feat_extr(torch.zeros(1, 3, 224, 224).to(dev))\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare features\n",
    "features_2 = test_out\n",
    "print((features_1 - features_2).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our feature extraction to process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of features\n",
    "num_features = feat_extr(dog_train_dataset[0][0].unsqueeze(0).to(dev)).numel()\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrices (num_samples x num_features)\n",
    "datasets = {\"train\": dog_train_dataset, \"val\": dog_val_dataset, \"test\": dog_test_dataset}\n",
    "features = {\"train\": torch.Tensor(len(dog_train_dataset), num_features),\n",
    "            \"val\":   torch.Tensor(len(dog_val_dataset), num_features),\n",
    "            \"test\":  torch.Tensor(len(dog_test_dataset), num_features)\n",
    "           }\n",
    "labels = {\"train\": torch.LongTensor(len(dog_train_dataset)),\n",
    "          \"val\":   torch.LongTensor(len(dog_val_dataset)),\n",
    "          \"test\":  torch.LongTensor(len(dog_test_dataset))\n",
    "         }\n",
    "# Fill the features for each split\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"Processing {split} split\")\n",
    "    # Process each sample in the split\n",
    "    for i in range(len(datasets[split])):\n",
    "        # Get sample\n",
    "        sample,label = datasets[split][i]\n",
    "        # Compute features\n",
    "        sample = sample.unsqueeze(0).to(dev)\n",
    "        feats = feat_extr(sample)\n",
    "        # Copy features\n",
    "        features[split][i] = feats\n",
    "        labels[split][i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data formatted as in our initial examples with linear regression and classification. However, we can still use the standard `DataLoader` interface, by wrapping our matrices as `TensorDataset` objects. In a `TensorDataset`, you can pass any kind of tensors as source data, and sample selection is performed by indexing the first dimension (in our case, rows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from torch.utils.data import TensorDataset\n",
    "# Prepare tensor datasets\n",
    "tensor_datasets = {\n",
    "    split: TensorDataset(features[split], labels[split]) for split in features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=tensor_datasets[\"train\"], batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=tensor_datasets[\"val\"],   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=tensor_datasets[\"test\"],  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a linear classifier on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = nn.Linear(num_features, num_classes)\n",
    "model = model.to(dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output\n",
    "model.eval()\n",
    "test_input = tensor_datasets[\"train\"][0][0].unsqueeze(0).to(dev)\n",
    "print(\"Model output size:\", model(test_input).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training history\n",
    "loss_history = {'train': [], 'val': [], 'test': []}\n",
    "accuracy_history = {'train': [], 'val': [], 'test': []}\n",
    "# Keep track of best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "test_accuracy_at_best_val = 0\n",
    "# Start training\n",
    "for epoch in range(100):\n",
    "    # Initialize accumulators for computing average loss/accuracy\n",
    "    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}\n",
    "    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}\n",
    "    # Process each split\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        # Set network mode\n",
    "        if split == \"train\":\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        # Process all data in split\n",
    "        for (input,target) in loaders[split]:\n",
    "            # Move to device\n",
    "            input = input.to(dev)\n",
    "            target = target.to(dev)\n",
    "            # Forward\n",
    "            output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Update loss sum\n",
    "            epoch_loss_sum[split] += loss.item()\n",
    "            epoch_loss_cnt[split] += 1\n",
    "            # Compute accuracy\n",
    "            _,pred = output.max(1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "            accuracy = correct/input.size(0)\n",
    "            # Update accuracy sum\n",
    "            epoch_accuracy_sum[split] += accuracy\n",
    "            epoch_accuracy_cnt[split] += 1\n",
    "            # Backward and optimize\n",
    "            if split == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    # Compute average epoch loss/accuracy\n",
    "    avg_train_loss = epoch_loss_sum[\"train\"]/epoch_loss_cnt[\"train\"]\n",
    "    avg_train_accuracy = epoch_accuracy_sum[\"train\"]/epoch_accuracy_cnt[\"train\"]\n",
    "    avg_val_loss = epoch_loss_sum[\"val\"]/epoch_loss_cnt[\"val\"]\n",
    "    avg_val_accuracy = epoch_accuracy_sum[\"val\"]/epoch_accuracy_cnt[\"val\"]\n",
    "    avg_test_loss = epoch_loss_sum[\"test\"]/epoch_loss_cnt[\"test\"]\n",
    "    avg_test_accuracy = epoch_accuracy_sum[\"test\"]/epoch_accuracy_cnt[\"test\"]\n",
    "    print(f\"Epoch: {epoch+1}, TL={avg_train_loss:.4f}, TA={avg_train_accuracy:.4f}, VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ŦL={avg_test_loss:.4f}, ŦA={avg_test_accuracy:.4f}\")\n",
    "    # Add to histories\n",
    "    loss_history[\"train\"].append(avg_train_loss)\n",
    "    loss_history[\"val\"].append(avg_val_loss)\n",
    "    loss_history[\"test\"].append(avg_test_loss)\n",
    "    accuracy_history[\"train\"].append(avg_train_accuracy)\n",
    "    accuracy_history[\"val\"].append(avg_val_accuracy)\n",
    "    accuracy_history[\"test\"].append(avg_test_accuracy)\n",
    "    # Check best validation\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        # Update best validation\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        test_accuracy_at_best_val = avg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy at best validation accuracy\n",
    "print(f\"Final test accuracy {test_accuracy_at_best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, loss_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, loss_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "x = torch.arange(1, len(loss_history[\"train\"])+1).numpy()\n",
    "plt.plot(x, accuracy_history[\"train\"], label=\"train\")\n",
    "plt.plot(x, accuracy_history[\"val\"], label=\"val\")\n",
    "plt.plot(x, accuracy_history[\"test\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get random sample from test set\n",
    "idx = random.randint(0, len(dog_test_dataset)-1)\n",
    "input, label = dog_test_dataset[idx]\n",
    "# Normalize and show image\n",
    "input_show = (input - input.min())/(input.max() - input.min())\n",
    "plt.imshow(input_show.permute(1,2,0).numpy())\n",
    "plt.axis('off')\n",
    "# Extract features\n",
    "input = feat_extr(input.unsqueeze(0).to(dev))\n",
    "# Predict class\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "_,pred = output.max(1)\n",
    "pred = pred.item()\n",
    "print(f\"Predicted: {pred} (correct: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
